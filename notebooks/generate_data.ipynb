{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea36023",
   "metadata": {},
   "source": [
    "## Notebook for generating the dataset for toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72daf1",
   "metadata": {},
   "source": [
    "In this notebook we generate the toy example data for sanity checking our methods. The data is $(x, y) \\in \\mathbb{R}^3 \\times \\{0, 1\\}$ for any training pair in the dataset. We train on datasets with both explanations $(x_1, x_2)$ and $(x_2, x_3)$ (and any permutation thereof) and test on seperate test sets that only include one of the explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a74f5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0a20d",
   "metadata": {},
   "source": [
    "Generating the training examples, currently we only use the simple classification function: $\\text{sign}(x_1 + x_2) > 0 \\Rightarrow 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35c74c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5617,  0.0576, -0.5617],\n",
       "         [ 0.0563, -0.6701,  0.0563],\n",
       "         [ 0.1022,  0.9445,  0.1022],\n",
       "         [-1.4113, -1.4094, -1.4113],\n",
       "         [ 0.8871, -0.0460,  0.8871]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1]], dtype=torch.int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_data(func, train_size: int = 10000):\n",
    "    training_examples = torch.randn((train_size, 3))\n",
    "    training_examples[:, 2] = training_examples[:, 0]\n",
    "    training_labels = func(training_examples).unsqueeze(dim=-1).int()\n",
    "\n",
    "    return training_examples, training_labels\n",
    "\n",
    "def simple_decs_func(training_examples: torch.Tensor):\n",
    "    return (training_examples[:, 0] + training_examples[:, 1]) > 0\n",
    "\n",
    "training_simple = gen_data(simple_decs_func)\n",
    "training_simple[0][:5], training_simple[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1291ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_sets(func, test_size: int = 1000):\n",
    "    expl_1_training = torch.randn((test_size, 3)) # explanation of (x1, x2)\n",
    "    expl_1_labels = func(expl_1_training).unsqueeze(dim=-1).int()\n",
    "    expl_1_training[:, 2] = torch.randn_like(expl_1_training[:, 2]) * 2 + 5 # rand out x3 so only x1, x2 can be used\n",
    "    \n",
    "    expl_2_training = torch.randn((test_size, 3)) # explanation of (x2, x3)\n",
    "    expl_2_labels = func(expl_2_training).unsqueeze(dim=-1).int()\n",
    "    expl_2_training[:, 2] = expl_2_training[:, 0]\n",
    "    expl_2_training[:, 0] = torch.randn_like(expl_1_training[:, 0]) * 2 + 5 # rand out x1 so only x3, x2 can be used\n",
    "    \n",
    "    return (expl_1_training, expl_1_labels), (expl_2_training, expl_2_labels)\n",
    "\n",
    "test_sets = gen_test_sets(simple_decs_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f9cae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5008,  0.5345,  3.5692],\n",
       "         [ 0.5979, -0.0629,  4.6133],\n",
       "         [-1.4677, -0.8215,  6.9923],\n",
       "         [ 0.1728,  2.3955,  1.4230],\n",
       "         [ 1.1203, -0.4027,  1.1848]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1]], dtype=torch.int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sets[0][0][:5], test_sets[0][1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d4e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .pt as its more optimized for torch tensors\n",
    "BASE_DIR = '../data/toy_example'\n",
    "save_path_simple = os.path.join(BASE_DIR, 'simple_func_dataset.pt')\n",
    "torch.save({'training_x': training_simple[0],\n",
    "            'training_y': training_simple[1],\n",
    "            'test1_x': test_sets[0][0],\n",
    "            'test1_y': test_sets[0][1],\n",
    "            'test2_x': test_sets[1][0],\n",
    "            'test2_y': test_sets[1][1],\n",
    "            }, save_path_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f7291",
   "metadata": {},
   "source": [
    "We can also try a more complicated function: $\\text{sin}(x_1 + x_2) > 0 \\Rightarrow 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50820b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_decs(training_examples):\n",
    "    return torch.sin(training_examples[:, 0] + training_examples[:, 2]) > 0\n",
    "\n",
    "training_sine = gen_data(sine_decs)\n",
    "test_sets = gen_test_sets(sine_decs)\n",
    "\n",
    "save_path_sine = os.path.join(BASE_DIR, 'sine_func_dataset.pt')\n",
    "torch.save({'training_x': training_sine[0],\n",
    "            'training_y': training_sine[1],\n",
    "            'test1_x': test_sets[0][0],\n",
    "            'test1_y': test_sets[0][1],\n",
    "            'test2_x': test_sets[1][0],\n",
    "            'test2_y': test_sets[1][1],\n",
    "            }, save_path_sine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386855bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.2298, -1.3915, -1.0504],\n",
       "         [ 5.3804, -0.6354, -0.9392],\n",
       "         [ 4.9189,  1.0054, -0.7379],\n",
       "         ...,\n",
       "         [ 9.0359, -0.2256,  0.0445],\n",
       "         [ 3.7391, -0.3364,  1.1681],\n",
       "         [ 5.3135,  0.0984, -0.6612]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sets[1][0], test_sets[1][1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da50119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 8])\n",
      "Adjacency shape: torch.Size([2, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 1., 0.],\n",
       "         [0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1.],\n",
       "         [1., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 1., 0.]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softmax, gumbel_softmax\n",
    "from sparse_generalization.layers.bern_mha import MultiHeadAttentionBern\n",
    "\n",
    "# Assuming your class MultiHeadAttentionBern is already defined and imported\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "embed_size = 8\n",
    "num_heads = 1\n",
    "dropout = 0.0\n",
    "temp = 0.5\n",
    "hard = True\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(batch_size, seq_len, embed_size, requires_grad=True)\n",
    "\n",
    "# Instantiate the layer\n",
    "mha_bern = MultiHeadAttentionBern(embed_size=embed_size,\n",
    "                                  num_heads=num_heads,\n",
    "                                  dropout=dropout,\n",
    "                                  temp=temp,\n",
    "                                  hard=hard)\n",
    "\n",
    "# Forward pass\n",
    "output, adjacency = mha_bern(x, x, x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)           # Expected: (batch_size, seq_len, embed_size)\n",
    "print(\"Adjacency shape:\", adjacency.shape)     # Expected: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "adjacency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-generalization (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
