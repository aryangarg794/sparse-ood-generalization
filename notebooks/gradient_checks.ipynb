{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import MultiheadAttention\n",
    "from sparse_generalization.layers.thresh_mha import MultiHeadAttentionThresh\n",
    "from sparse_generalization.losses.sparse_loss import L1SparsityWeights, L1SparsityAdjacency\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "logits = torch.randn((4, 3, 2), requires_grad=True)\n",
    "\n",
    "mha = MultiHeadAttentionThresh(embed_size=2, num_heads=1, batch_first=True)\n",
    "mha2 = MultiheadAttention(embed_dim=2, num_heads=1, batch_first=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([logits], lr=0.01)\n",
    "weight = 1\n",
    "loss_func = L1SparsityWeights()\n",
    "\n",
    "print(\"Initial logits:\")\n",
    "print(logits.data)\n",
    "\n",
    "def copy_weights(torch_mha, my_mha):\n",
    "    Wq, Wk, Wv = torch.chunk(torch_mha.in_proj_weight.data, 3, dim=0)\n",
    "    bq, bk, bv = torch.chunk(torch_mha.in_proj_bias.data, 3, dim=0)\n",
    "    my_mha.queries.weight.data = Wq.clone()\n",
    "    my_mha.queries.bias.data   = bq.clone()\n",
    "    my_mha.keys.weight.data    = Wk.clone()\n",
    "    my_mha.keys.bias.data      = bk.clone()\n",
    "    my_mha.values.weight.data  = Wv.clone()\n",
    "    my_mha.values.bias.data    = bv.clone()\n",
    "    my_mha.projection.weight.data = torch_mha.out_proj.weight.data.clone()\n",
    "    my_mha.projection.bias.data   = torch_mha.out_proj.bias.data.clone()\n",
    "\n",
    "A_sums = []\n",
    "\n",
    "copy_weights(mha2, mha)\n",
    "\n",
    "for step in range(1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out, attn = mha(logits, logits, logits)\n",
    "\n",
    "    A = attn  \n",
    "    loss = loss_func(A)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    A_sum_value = (A > 0.1).detach().sum(dim=(1, 2)).float().mean().item()\n",
    "    A_sums.append(A_sum_value)\n",
    "\n",
    "    # print(f\"\\n--- Step {step} ---\")\n",
    "    # print(\"Loss:\", loss.item())\n",
    "    # print(\"Grad magnitude:\", logits.grad)\n",
    "    # print(\"Logits before step:\\n\", logits.data)\n",
    "    # print(\"Adjacency A:\\n\", A.detach())\n",
    "    # print(\"Adjacency A (sum):\", A_sum_value)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(\"Logits after step:\\n\", logits.data)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(A_sums)\n",
    "plt.title(\"Sparsity Curve: Num of weights above 0.1\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Mean over batch\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sparse_generalization.layers.bern_mha import MultiHeadAttentionBern\n",
    "from sparse_generalization.losses.sparse_loss import L1SparsityAdjacency\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Learnable logits\n",
    "logits = torch.randn((4, 3, 2), requires_grad=True)\n",
    "\n",
    "mha = MultiHeadAttentionBern(embed_size=2, num_heads=1, hard=True, temp=0.5)\n",
    "\n",
    "optimizer = torch.optim.Adam([logits], lr=0.1)\n",
    "weight = 1\n",
    "loss_func = L1SparsityAdjacency()\n",
    "\n",
    "print(\"Initial logits:\")\n",
    "print(logits.data)\n",
    "\n",
    "A_sums = [] \n",
    "\n",
    "for step in range(50):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out, attn = mha(logits, logits, logits)\n",
    "\n",
    "    A = attn \n",
    "\n",
    "    loss = weight * loss_func(A)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    A_sum_value = A.detach().sum(dim=(1, 2)).mean().item()\n",
    "    A_sums.append(A_sum_value)\n",
    "\n",
    "    print(f\"\\n--- Step {step} ---\")\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"Grad magnitude:\", logits.grad)\n",
    "    print(\"Logits before step:\\n\", logits.data)\n",
    "    print(\"Adjacency A:\\n\", A.detach())\n",
    "    print(\"Adjacency A (sum):\", A_sum_value)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(A_sums)\n",
    "plt.title(\"Sparsity Curve: A.sum over Optimization Steps\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Mean A.sum()\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
