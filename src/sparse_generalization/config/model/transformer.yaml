_target_: sparse_generalization.models.transformer.TransformerLit

embed_size: 3
out_dim: 1 
hidden_dims: [64, 128, 64]
use_grid: true
model_dim: 16
num_heads: 1
include_sparsity: true
l1_weight: 0.01
residual: false
positional_encoding: true

mha_layer:
  _partial_: true
  _target_: sparse_generalization.layers.bern_mha.MultiHeadAttentionBern
sparse_loss: 
  _partial_: true
  _target_: sparse_generalization.losses.sparse_loss.L1SparsityAdjacency