_target_: sparse_generalization.models.transformer.TransformerLit

embed_size: 3
out_dim: 1 
hidden_dims: [64, 128, 64]
use_grid: true
model_dim: 64
num_feature_layers: 3
num_heads: 1
include_sparsity: true
l1_weight: 0.1
residual: false
positional_encoding: true
lr: 1e-3
k: 4000
lagrangian: false

mha_layer:
  _partial_: true
  _target_: sparse_generalization.layers.bern_mha.MultiHeadAttentionBern
sparse_loss: 
  _partial_: true
  _target_: sparse_generalization.losses.sparse_loss.L1SparsityAdjacency