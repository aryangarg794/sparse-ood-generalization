_target_: sparse_generalization.models.transformer.TransformerLit

embed_size: 3
out_dim: 1 
hidden_dims: [64, 128, 64]
use_grid: true
model_dim: 64
num_feature_layers: 3
num_heads: 1
include_sparsity: true
l1_weight: 0.01
residual: false
positional_encoding: true
lr: 1e-3
k: 4032
lagrangian: false
start_lambda: 1e7
target_loss: 0.05 
cma: 0.9
step_size: 1e-1
noisy_grads: false
eta: 1.0
gamma: 0.55
noisy_bern: false
beta1: 0.99
beta2: 0.999
foopt: false

mha_layer:
  _partial_: true
  _target_: sparse_generalization.layers.bern_mha.MultiHeadAttentionBern
sparse_loss: 
  _partial_: true
  _target_: sparse_generalization.losses.sparse_loss.L1SparsityAdjacency